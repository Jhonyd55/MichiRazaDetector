{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEC73GPmyHcSwiPGL5xm0h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhonyd55/MichiRazaDetector/blob/main/MichiRazaDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title instalar complementos si no estan instalados\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package_name):\n",
        "    if importlib.util.find_spec(package_name) is None:\n",
        "      print(f\"üîß Instalando {package_name}...\")\n",
        "      subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "      print(f\"{package_name} instalado con √©xito.\")\n",
        "    else:\n",
        "      print(f\"{package_name} ya est√° instalado.\")\n",
        "\n",
        "#package_name = \"google_images_download\"\n",
        "package_name = \"bing-image-downloader\"\n",
        "install_package(package_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "ote5jMRKyFif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "folder = 'dataset'\n",
        "\n",
        "if os.path.exists(folder):\n",
        "    shutil.rmtree(folder)\n",
        "    print(f\"‚úÖ Carpeta '{folder}' eliminada correctamente.\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è La carpeta '{folder}' no existe.\")\n"
      ],
      "metadata": {
        "id": "TgPbMxnFQfjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Razas de gatos segun la TICA\n",
        "\n",
        "#search_queries = ['Abyssinian cat', 'Bengal cat', 'Persian cat']\n",
        "\n",
        "search_queries = [\n",
        "    \"Abyssinian cat\",\n",
        "    \"American Bobtail cat\",\n",
        "    \"American Curl cat\",\n",
        "    \"American Shorthair cat\",\n",
        "    \"American Wirehair cat\",\n",
        "    \"Balinese cat\",\n",
        "    \"Bengal cat\",\n",
        "    \"Birman cat\",\n",
        "    \"Bombay cat\",\n",
        "    \"British Longhair cat\",\n",
        "    \"British Shorthair cat\",\n",
        "    \"Burmese cat\",\n",
        "    \"Burmilla cat\",\n",
        "    \"Chartreux cat\",\n",
        "    \"Colorpoint Shorthair cat\",\n",
        "    \"Cornish Rex cat\",\n",
        "    \"Cymric cat\",\n",
        "    \"Devon Rex cat\",\n",
        "    \"Egyptian Mau cat\",\n",
        "    \"European Burmese cat\",\n",
        "    \"Exotic Shorthair cat\",\n",
        "    \"Havana Brown cat\",\n",
        "    \"Himalayan cat\",\n",
        "    \"Japanese Bobtail cat\",\n",
        "    \"Khao Manee cat\",\n",
        "    \"Korat cat\",\n",
        "    \"LaPerm cat\",\n",
        "    \"Lykoi cat\",\n",
        "    \"Maine Coon cat\",\n",
        "    \"Manx cat\",\n",
        "    \"Norwegian Forest cat\",\n",
        "    \"Ocicat cat\",\n",
        "    \"Oriental cat\",\n",
        "    \"Persian cat\",\n",
        "    \"Pixiebob cat\",\n",
        "    \"Ragamuffin cat\",\n",
        "    \"Ragdoll cat\",\n",
        "    \"Russian Blue cat\",\n",
        "    \"Savannah cat\",\n",
        "    \"Scottish Fold cat\",\n",
        "    \"Selkirk Rex cat\",\n",
        "    \"Siamese cat\",\n",
        "    \"Siberian cat\",\n",
        "    \"Singapura cat\",\n",
        "    \"Snowshoe cat\",\n",
        "    \"Somali cat\",\n",
        "    \"Sphynx cat\",\n",
        "    \"Tonkinese cat\",\n",
        "    \"Toyger cat\",\n",
        "    \"Turkish Angora cat\",\n",
        "    \"Turkish Van cat\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "pV8b_zOeOQZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de6b1c98"
      },
      "source": [
        "license_filters = [\n",
        "    \"license-cc_publicdomain\", # Dominio P√∫blico\n",
        "    \"license-cc_attribution\", # Atribuci√≥n (CC BY)\n",
        "    \"license-cc_sharealike\", # Compartir Igual (CC BY-SA)\n",
        "    \"license-cc_noncommercial\", # No Comercial (CC BY-NC)\n",
        "    \"license-cc_noncommercial_sharealike\", # No Comercial - Compartir Igual (CC BY-NC-SA)\n",
        "    \"license-cc_noncommercial_noderivatives\", # No Comercial - Sin Obras Derivadas (CC BY-NC-ND)\n",
        "    \"license-cc_noderivatives\" # Sin Obras Derivadas (CC BY-ND)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from bing_image_downloader import downloader\n",
        "\n",
        "def download_filtered_images(search_queries, license_filters, base_dir=\"dataset\", limit=100, timeout_per_license=300): # tiempo de espera por licencia en segundos (5 minutos)\n",
        "    for query in search_queries:\n",
        "        query_dir = os.path.join(base_dir, query)#.replace(\" \", \"_\"))\n",
        "        downloaded_count = 0\n",
        "\n",
        "        print(f\"‚¨áÔ∏è Descargando im√°genes de: {query} en carpeta {query_dir}\")\n",
        "        # Verificar si la carpeta ya existe y tiene suficientes im√°genes\n",
        "        if os.path.exists(query_dir):\n",
        "            downloaded_count = len([f for f in os.listdir(query_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            print(f\"‚¨áÔ∏è Cantidad de im√°genes: {downloaded_count} en carpeta\")\n",
        "            if downloaded_count >= limit:\n",
        "                print(f\"‚úÖ Ya existe {query_dir} con {downloaded_count} im√°genes, omitiendo descarga.\")\n",
        "                continue\n",
        "        else:\n",
        "              #downloaded_count = len([f for f in os.listdir(query_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "              #print(f\"Continuando la descarga para {query_dir}, {downloaded_count} im√°genes ya existen.\")\n",
        "              ###\n",
        "              # Iterar a trav√©s de los filtros de licencia para la consulta actual\n",
        "              for license_filter in license_filters:\n",
        "                print(f\"  Probando filtro de licencia: {license_filter}\")\n",
        "                start_time = time.time()\n",
        "                initial_count = downloaded_count # Mantener un registro de las im√°genes antes de este intento de licencia\n",
        "\n",
        "                try:\n",
        "                    downloader.download(\n",
        "                        query,\n",
        "                        limit=limit - downloaded_count, # Descargar im√°genes restantes\n",
        "                        output_dir=base_dir,\n",
        "                        adult_filter_off=True,\n",
        "                        force_replace=False,\n",
        "                        timeout=60, # Tiempo de espera para la descarga de im√°genes individuales\n",
        "                        filter=license_filter,\n",
        "                        verbose=False # Reducir la verbosidad para evitar demasiada salida\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error durante la descarga con la licencia {license_filter}: {e}\")\n",
        "\n",
        "                # üîç Eliminar archivos que no sean .jpg, .png, o .jpeg en el directorio de la consulta\n",
        "                if os.path.exists(query_dir):\n",
        "                    for fname in os.listdir(query_dir):\n",
        "                        if not (fname.lower().endswith(\".jpg\") or fname.lower().endswith(\".png\") or fname.lower().endswith(\".jpeg\")):\n",
        "                            try:\n",
        "                                os.remove(os.path.join(query_dir, fname))\n",
        "                                print(f\"üóëÔ∏è Eliminado archivo no compatible: {fname} en {query_dir}\")\n",
        "                            except Exception as e:\n",
        "                                print(f\"‚ùå Error al eliminar el archivo {fname}: {e}\")\n",
        "\n",
        "\n",
        "                # Actualizar el n√∫mero de im√°genes descargadas despu√©s del intento de descarga\n",
        "                if os.path.exists(query_dir):\n",
        "                    downloaded_count = len([f for f in os.listdir(query_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "\n",
        "                print(f\"‚¨áÔ∏è Cantidad de im√°genes despu√©s de la limpieza: {downloaded_count} en carpeta\")\n",
        "                # Verificar si se han descargado suficientes im√°genes\n",
        "                if downloaded_count >= limit:\n",
        "                    print(f\"‚úÖ Descargadas {downloaded_count} im√°genes para {query}. Pasando a la siguiente consulta.\")\n",
        "                    break # Pasar a la siguiente consulta si se alcanza el l√≠mite\n",
        "\n",
        "                # Verificar si se ha alcanzado el tiempo de espera para esta licencia y no se han descargado nuevas im√°genes\n",
        "                elapsed_time = time.time() - start_time\n",
        "                if elapsed_time >= timeout_per_license and downloaded_count == initial_count:\n",
        "                    print(f\"  Tiempo de espera ({timeout_per_license}s) alcanzado para la licencia {license_filter} sin nuevas im√°genes. Cambiando a la siguiente licencia.\")\n",
        "                    # Continuar con el siguiente filtro de licencia\n",
        "\n",
        "\n",
        "        # Despu√©s de intentar todas las licencias, verificar si se alcanz√≥ el l√≠mite.\n",
        "        if downloaded_count < limit:\n",
        "            print(f\"‚ö†Ô∏è No se pudieron descargar {limit} im√°genes para {query} despu√©s de probar todas las licencias. Descargadas {downloaded_count}.\")\n",
        "                 ###\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Ahora llama a la funci√≥n con la lista de consultas de b√∫squeda y filtros de licencia\n",
        "# Aseg√∫rate de que la lista license_filters est√© definida en una celda anterior\n",
        "\n",
        "# Define los filtros de licencia para ciclar\n",
        "# license_filters = [\n",
        "#     \"license-cc_publicdomain\",\n",
        "#     \"license-cc_attribution\",\n",
        "#     \"license-cc_sharealike\",\n",
        "#     \"license-cc_noncommercial\",\n",
        "#     \"license-cc_noncommercial_sharealike\",\n",
        "#     \"license-cc_noncommercial_noderivatives\",\n",
        "#     \"license-cc_noderivatives\"\n",
        "# ]\n",
        "\n",
        "download_filtered_images(search_queries, license_filters, limit=100, timeout_per_license=300) # 5 minutos de tiempo de espera por licencia"
      ],
      "metadata": {
        "id": "-jcwtnlnOfJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imagenes antes de la Selecci√≥n\n",
        "analysis_df = analyze_dataset_structure('dataset')\n",
        "if analysis_df is not None:\n",
        "  display(analysis_df)"
      ],
      "metadata": {
        "id": "PEAPhkEDnLbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5daf99a5"
      },
      "source": [
        "# @title Herramienta para eliminar las imagenes que no sirven para el entrenamiento\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "from PIL import Image as PILImage\n",
        "from io import BytesIO\n",
        "\n",
        "# Carpeta base del dataset\n",
        "dataset_path = 'dataset'\n",
        "\n",
        "# Dictionary to store selected images for deletion\n",
        "selected_images = {}\n",
        "\n",
        "# Obtener las razas disponibles\n",
        "def get_race_folders(base_path):\n",
        "    return [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
        "\n",
        "# Mostrar im√°genes con bot√≥n de eliminar y checkbox de selecci√≥n\n",
        "def show_images_grid(race):\n",
        "    clear_output(wait=True)\n",
        "    folder_path = os.path.join(dataset_path, race)\n",
        "    image_paths = glob.glob(os.path.join(folder_path, '*'))\n",
        "\n",
        "    if not image_paths:\n",
        "        print(f\"‚ö†Ô∏è No hay im√°genes en {race}\")\n",
        "        return\n",
        "\n",
        "    print(f\"üñºÔ∏è Selecciona las im√°genes a eliminar y haz clic en üóëÔ∏è para eliminarlas. Raza: {race}\")\n",
        "\n",
        "    items = []\n",
        "    # Clear previous selections when changing race\n",
        "    selected_images.clear()\n",
        "\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            # Cargar imagen\n",
        "            with open(path, 'rb') as f:\n",
        "                img = PILImage.open(f).convert(\"RGB\")\n",
        "                img.thumbnail((100, 100))\n",
        "                buffer = BytesIO()\n",
        "                img.save(buffer, format='JPEG')\n",
        "                img_data = buffer.getvalue()\n",
        "\n",
        "            # Widget de imagen\n",
        "            image_widget = widgets.Image(\n",
        "                value=img_data,\n",
        "                format='jpg',\n",
        "                width=220,\n",
        "                height=220\n",
        "            )\n",
        "\n",
        "            # Checkbox para seleccionar\n",
        "            select_checkbox = widgets.Checkbox(\n",
        "                value=False,\n",
        "                description='Seleccionar',\n",
        "                indent=False,\n",
        "                layout=widgets.Layout(width='auto')\n",
        "            )\n",
        "\n",
        "            # Associate checkbox with image path\n",
        "            selected_images[path] = select_checkbox\n",
        "\n",
        "            # Bot√≥n para eliminar (will be modified later to delete selected)\n",
        "            # For now, keep the single delete button functionality as a placeholder\n",
        "            delete_button = widgets.Button(\n",
        "                description='üóëÔ∏è',\n",
        "                layout=widgets.Layout(width='220px'),\n",
        "                tooltip=path,\n",
        "                button_style='danger'\n",
        "            )\n",
        "\n",
        "            def on_click(b, path=path):\n",
        "                try:\n",
        "                    os.remove(path)\n",
        "                    print(f\"‚úÖ Imagen eliminada: {os.path.basename(path)}\")\n",
        "                    show_images_grid(race)  # Recargar la grilla\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "            # delete_button.on_click(on_click) # Temporarily disable single delete\n",
        "\n",
        "            # Agrupar imagen + checkbox + button\n",
        "            box = widgets.VBox([image_widget, select_checkbox, delete_button])\n",
        "            items.append(box)\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Mostrar en grilla\n",
        "    grid = widgets.GridBox(\n",
        "        children=items,\n",
        "        layout=widgets.Layout(grid_template_columns=\"repeat(3, 220px)\")\n",
        "    )\n",
        "\n",
        "    # Add a master delete button for selected images (will be implemented in the next subtask)\n",
        "    master_delete_button = widgets.Button(\n",
        "        description='Eliminar Seleccionadas üóëÔ∏è',\n",
        "        button_style='danger',\n",
        "        layout=widgets.Layout(width='auto')\n",
        "    )\n",
        "\n",
        "    # display(race_dropdown, grid) # Modify display to include master delete button\n",
        "    display(race_dropdown, master_delete_button, grid)\n",
        "\n",
        "\n",
        "# Dropdown de raza\n",
        "race_dropdown = widgets.Dropdown(\n",
        "    options=get_race_folders(dataset_path),\n",
        "    description='Raza:',\n",
        ")\n",
        "race_dropdown.observe(lambda change: show_images_grid(change['new']), names='value')\n",
        "\n",
        "# Mostrar inicial\n",
        "display(race_dropdown)\n",
        "show_images_grid(race_dropdown.value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title preprocesamiento\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "\n",
        "# üìç Ruta original y de salida\n",
        "ORIG_DIR = 'dataset'\n",
        "DEST_DIR = 'processed_dataset'\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "# üßΩ Limpiar carpeta de salida si ya existe\n",
        "if os.path.exists(DEST_DIR):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.makedirs(DEST_DIR, exist_ok=True)\n",
        "\n",
        "# üîÅ Funci√≥n para preprocesar una imagen\n",
        "def preprocess_and_save(src_path, dest_path):\n",
        "    try:\n",
        "        img = Image.open(src_path).convert('RGB')\n",
        "        img = img.resize(IMG_SIZE)\n",
        "        img.save(dest_path)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error con {src_path}: {e}\")\n",
        "\n",
        "# üîÄ Divisi√≥n y preprocesamiento\n",
        "for class_name in os.listdir(ORIG_DIR):\n",
        "    class_path = os.path.join(ORIG_DIR, class_name)\n",
        "    if not os.path.isdir(class_path): continue\n",
        "\n",
        "    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    train_files, temp_files = train_test_split(images, train_size=0.7, random_state=42)\n",
        "    val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)  # 15/15\n",
        "\n",
        "    split_data = {\n",
        "        'train': train_files,\n",
        "        'val': val_files,\n",
        "        'test': test_files\n",
        "    }\n",
        "\n",
        "    for split, files in split_data.items():\n",
        "        out_dir = os.path.join(DEST_DIR, split, class_name)\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "        for file in files:\n",
        "            src = os.path.join(class_path, file)\n",
        "            dst = os.path.join(out_dir, file)\n",
        "            preprocess_and_save(src, dst)\n",
        "\n",
        "print(\"‚úÖ Preprocesamiento completo y dataset dividido.\")\n"
      ],
      "metadata": {
        "id": "UPYYZIglfBWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title üß† Red Preentrenada **MobileNetV2**\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# Tama√±o de entrada y batch\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "\n",
        "# Ruta del dataset procesado\n",
        "DATASET_DIR = 'processed_dataset'\n",
        "\n",
        "# Cargar datos con generadores\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    os.path.join(DATASET_DIR, 'train'),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    os.path.join(DATASET_DIR, 'val'),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_gen = test_datagen.flow_from_directory(\n",
        "    os.path.join(DATASET_DIR, 'test'),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# N√∫mero de clases\n",
        "num_classes = train_gen.num_classes\n",
        "\n",
        "# Base preentrenada MobileNetV2 sin las capas superiores\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Congelar capas del modelo base\n",
        "base_model.trainable = False\n",
        "\n",
        "# A√±adir capas finales\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Modelo final\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compilar\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Entrenar\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=EPOCHS\n",
        ")\n"
      ],
      "metadata": {
        "id": "P6kDpI-Pf3DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üìä Evaluar el modelo\n",
        "loss, acc = model.evaluate(test_gen)\n",
        "print(f\"üìà Precisi√≥n en test: {acc:.2%}\")"
      ],
      "metadata": {
        "id": "Ku3ekHuVgWSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generar un algoritmo  para evaluar la matriz de confusion y la metrica con una buena visualizcion del proyecto\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Obtener predicciones del conjunto de prueba\n",
        "test_pred_raw = model.predict(test_gen)\n",
        "test_pred = np.argmax(test_pred_raw, axis=1)\n",
        "\n",
        "# Obtener etiquetas verdaderas del conjunto de prueba\n",
        "test_true_labels = test_gen.classes\n",
        "\n",
        "# Obtener los nombres de las clases en el mismo orden que el generador\n",
        "class_names = list(test_gen.class_indices.keys())\n",
        "\n",
        "# Calcular la matriz de confusi√≥n\n",
        "conf_matrix = confusion_matrix(test_true_labels, test_pred)\n",
        "\n",
        "# Visualizar la matriz de confusi√≥n\n",
        "plt.figure(figsize=(20, 15))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Clase Predicha')\n",
        "plt.ylabel('Clase Verdadera')\n",
        "plt.title('Matriz de Confusi√≥n')\n",
        "plt.show()\n",
        "\n",
        "# Generar el reporte de clasificaci√≥n\n",
        "print(\"\\nüìä Reporte de Clasificaci√≥n:\")\n",
        "print(classification_report(test_true_labels, test_pred, target_names=class_names))\n",
        "\n",
        "# Visualizar la precisi√≥n y p√©rdida durante el entrenamiento\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Precisi√≥n\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Precisi√≥n Entrenamiento')\n",
        "plt.plot(history.history['val_accuracy'], label='Precisi√≥n Validaci√≥n')\n",
        "plt.title('Precisi√≥n del Modelo')\n",
        "plt.ylabel('Precisi√≥n')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.legend()\n",
        "\n",
        "# P√©rdida\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='P√©rdida Entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='P√©rdida Validaci√≥n')\n",
        "plt.title('P√©rdida del Modelo')\n",
        "plt.ylabel('P√©rdida')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pgJU9qCzo0n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# 1. Obtener predicciones y etiquetas reales del conjunto de prueba\n",
        "test_gen.reset() # Reiniciar el generador para asegurar el orden\n",
        "y_pred_probs = model.predict(test_gen)\n",
        "y_true = test_gen.classes # Obtener las etiquetas verdaderas\n",
        "\n",
        "# Binarizar las etiquetas verdaderas para cada clase\n",
        "y_true_binarized = label_binarize(y_true, classes=np.arange(num_classes))\n",
        "\n",
        "# 2. Calcular la curva ROC y el AUC para cada clase\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "for i in range(num_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_binarized[:, i], y_pred_probs[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# 3. Graficar las curvas ROC en grupos de 10\n",
        "classes_per_plot = 10\n",
        "num_plots = (num_classes + classes_per_plot - 1) // classes_per_plot\n",
        "\n",
        "class_names = list(test_gen.class_indices.keys())\n",
        "\n",
        "for plot_idx in range(num_plots):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    start_idx = plot_idx * classes_per_plot\n",
        "    end_idx = min((plot_idx + 1) * classes_per_plot, num_classes)\n",
        "\n",
        "    for i in range(start_idx, end_idx):\n",
        "        plt.plot(fpr[i], tpr[i], lw=2,\n",
        "                 label='Curva ROC de la clase {0} (AUC = {1:0.2f})'\n",
        "                 ''.format(class_names[i], roc_auc[i]))\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
        "    plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
        "    plt.title(f'Curva ROC y AUC para clases {start_idx+1} a {end_idx}')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Calcular AUC macro y micro promedio (opcional) - Esta parte permanece igual\n",
        "all_y_true = y_true_binarized.ravel()\n",
        "all_y_pred_probs = y_pred_probs.ravel()\n",
        "\n",
        "# Macro promedio\n",
        "# Necesita manejar el caso multi-clase para el promedio macro\n",
        "# fpr_macro, tpr_macro, _ = roc_curve(all_y_true, all_y_pred_probs)\n",
        "# roc_auc_macro = auc(fpr_macro, tpr_macro)\n",
        "\n",
        "# print(f\"\\nAUC Promedio Macro: {roc_auc_macro:.2f}\")\n",
        "\n",
        "# Micro promedio (m√°s apropiado para multi-clase)\n",
        "fpr_micro, tpr_micro, _ = roc_curve(y_true_binarized.ravel(), y_pred_probs.ravel())\n",
        "roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
        "\n",
        "print(f\"\\nAUC Promedio Micro: {roc_auc_micro:.2f}\")"
      ],
      "metadata": {
        "id": "Im74EBTnReIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Busca una imagen random de validaci√≥n y la eval√∫a\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Funci√≥n para predecir una imagen aleatoria de validaci√≥n\n",
        "def predict_random_image_from_validation():\n",
        "    val_dir = os.path.join(DATASET_DIR, 'val')\n",
        "    class_names = os.listdir(val_dir)\n",
        "\n",
        "    # Elegir clase y archivo aleatorio\n",
        "    true_class = random.choice(class_names)\n",
        "    img_file = random.choice(os.listdir(os.path.join(val_dir, true_class)))\n",
        "    img_path = os.path.join(val_dir, true_class, img_file)\n",
        "\n",
        "    # Cargar imagen\n",
        "    img = image.load_img(img_path, target_size=IMG_SIZE)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = img_array / 255.0  # Normalizar\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Lote de 1\n",
        "\n",
        "    # Predicci√≥n\n",
        "    preds = model.predict(img_array)[0]\n",
        "    top_idx = np.argmax(preds)\n",
        "    pred_class = train_gen.class_indices\n",
        "    label_map = {v: k for k, v in pred_class.items()}\n",
        "    predicted_class = label_map[top_idx]\n",
        "    confidence = preds[top_idx]\n",
        "\n",
        "    # Mostrar resultado\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"‚úÖ Real: {true_class}\\nüîç Predicho: {predicted_class} ({confidence:.2%})\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "U4X1UusKgwOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_random_image_from_validation()\n"
      ],
      "metadata": {
        "id": "iPzFuuE4gym8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "package_name = \"gradio\"\n",
        "install_package(package_name)"
      ],
      "metadata": {
        "id": "DehO1Myei7Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Diccionario para mapear √≠ndice ‚Üí nombre de clase\n",
        "idx_to_class = {v: k for k, v in train_gen.class_indices.items()}\n",
        "\n",
        "# Funci√≥n de predicci√≥n para Gradio\n",
        "def predict_image(img):\n",
        "    from tensorflow.keras.preprocessing import image as keras_image\n",
        "\n",
        "    img = img.resize(IMG_SIZE)\n",
        "    img_array = keras_image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    predictions = model.predict(img_array)[0]\n",
        "\n",
        "    # Crear diccionario {raza: probabilidad}\n",
        "    result = {idx_to_class[i]: float(f\"{p:.4f}\") for i, p in enumerate(predictions)}\n",
        "\n",
        "    return result\n",
        "\n",
        "# Crear interfaz\n",
        "iface = gr.Interface(\n",
        "    fn=predict_image,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=gr.Label(num_top_classes=3),\n",
        "    title=\"üê± Michi Raza Detector\",\n",
        "    description=\"Sube una imagen de un gato para identificar su raza. Muestra las razas m√°s probables.\"\n",
        ")\n",
        "\n",
        "# Ejecutar interfaz\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "id": "Hmvixi6_jBSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Para Guardar y descargar el modelo entrenado\n",
        "\n",
        "model.save(\"michi_model.h5\")\n",
        "\n"
      ],
      "metadata": {
        "id": "E9EJkIgxWBte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Para convertirlo en una version para usar en un proyecto de android\n",
        "import tensorflow as tf\n",
        "\n",
        "# Cargar el modelo .h5\n",
        "loaded_model = tf.keras.models.load_model(\"michi_model.h5\")\n",
        "\n",
        "# Convertir a TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Guardar el archivo .tflite\n",
        "with open(\"michi_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n"
      ],
      "metadata": {
        "id": "zzIl86hgWQOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title Descargar el **modelo**\n",
        "from google.colab import files\n",
        "files.download(\"michi_model.tflite\")\n"
      ],
      "metadata": {
        "id": "pa4tG3MgWdX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title descargar dataset\n",
        "import shutil\n",
        "\n",
        "# Comprime la carpeta dataset en un archivo ZIP\n",
        "shutil.make_archive(\"dataset_gatitos\", 'zip', \"dataset\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5e9Nz8fMWyGQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}